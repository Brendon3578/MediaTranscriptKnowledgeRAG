# Media Search Engine with RAG (.NET + Whisper + pgvector)

## üìå Descri√ß√£o do Projeto

Este projeto consiste em uma plataforma backend robusta desenvolvida em .NET para ingest√£o, processamento e consulta sem√¢ntica de arquivos de m√≠dia (√°udio e v√≠deo). O sistema busca simplificar a consulta de conte√∫do de v√≠deos e √°udios, permitindo que usu√°rios fa√ßam perguntas em linguagem natural e obtenham respostas baseadas no conte√∫do falado nos v√≠deos, com refer√™ncia temporal precisa.

A solu√ß√£o utiliza uma abordagem 100% local (On-Premise) para Intelig√™ncia Artificial, empregando Whisper para transcri√ß√£o e Ollama para gera√ß√£o de embeddings e respostas (RAG), garantindo privacidade de dados e elimina√ß√£o de custos com APIs de terceiros, mas pode a arquitetura pode ser reaproveitada para a utiliza√ß√£o de servi√ßos de AI externos.

## üèó Arquitetura da Solu√ß√£o

O sistema foi desenhado seguindo uma **Arquitetura Orientada a Eventos (Event-Driven Architecture)**, desacoplando os processos de ingest√£o, processamento pesado (transcri√ß√£o/embeddings) e consulta.

* **APIs e Workers:** A solu√ß√£o √© dividida em APIs (Upload e Query) para intera√ß√£o s√≠ncrona com o usu√°rio e Workers (Transcription e Embedding) para processamento ass√≠ncrono em background.
* **Mensageria:** O **RabbitMQ** atua como arcabou√ßo de comunica√ß√£o, garantindo que cada etapa do pipeline seja acionada por eventos de dom√≠nio (`MediaUploadedEvent`, `MediaTranscribedEvent`), proporcionando escalabilidade e resili√™ncia.
* **Persist√™ncia:** O **PostgreSQL** √© utilizado como banco de dados central, utilizando a extens√£o **pgvector** para armazenamento e consulta eficiente de vetores de alta dimens√£o.

## üîÑ Fluxo de Funcionamento

O pipeline de processamento segue as seguintes etapas:

1. **Upload:** O usu√°rio envia um arquivo de m√≠dia (√°udio/v√≠deo) para a `Upload.Api`, selecionando opcionalmente o modelo de transcri√ß√£o (ex: Medium, Large). O arquivo √© armazenado e um evento `MediaUploadedEvent` √© publicado.
2. **Transcri√ß√£o:** O `Transcription.Worker` consome o evento, extrai o √°udio e utiliza o **Whisper.NET** (com modelos GGML locais) para transcrever o conte√∫do. O texto √© segmentado por tempo e persistido, gerando um evento `MediaTranscribedEvent`.
3. **Embeddings:** O `Embedding.Worker` reage ao evento de transcri√ß√£o, processa cada segmento de texto utilizando o **Ollama** para gerar vetores num√©ricos (embeddings) que representam o significado sem√¢ntico do trecho.
4. **Indexa√ß√£o:** Os vetores gerados s√£o armazenados na tabela `embeddings` do PostgreSQL, prontos para busca vetorial.
5. **Consulta (RAG):** Na `Query.Api`, a pergunta do usu√°rio √© convertida em vetor. O sistema realiza uma busca por similaridade no banco, recupera os segmentos mais relevantes e utiliza um LLM (via Ollama) para gerar uma resposta contextualizada.

## üöÄ Tecnologias Utilizadas

* **Linguagem Principal**: [C# .NET 8](https://dotnet.microsoft.com/)
* **APIs & Workers**: ASP.NET Core Web API, Background Services
* **Mensageria**: [RabbitMQ](https://www.rabbitmq.com/) (Event-Driven Architecture)
* **Banco de Dados**: [PostgreSQL](https://www.postgresql.org/)
* **Busca Vetorial**: [pgvector](https://github.com/pgvector/pgvector)
* **ORM / Data Access**: Entity Framework Core & [Dapper](https://github.com/DapperLib/Dapper)
* **Biblioteca de IA & LLM**: [Microsoft.Extensions.AI](https://devblogs.microsoft.com/dotnet/introducing-microsoft-extensions-ai-preview/)
* **Modelos Locais**: [Ollama](https://ollama.com/) (phi3:mini, bge-M3)
* **Transcri√ß√£o**: [Whisper.net](https://github.com/sandrohanea/whisper.net)
* **Processamento de M√≠dia**: [FFmpeg](https://ffmpeg.org/)
* **Infraestrutura**: Docker & Docker Compose

## Modelo de Dados (Vis√£o Geral)

O banco de dados foi modelado para suportar o fluxo de RAG com rastreabilidade:

* **media:** Armazena metadados dos arquivos originais (caminho, tamanho, tipo, data de upload).
* **transcriptions:** Cont√©m o registro da transcri√ß√£o completa, incluindo m√©tricas como modelo utilizado e tempo de processamento.
* **transcription_segments:** Tabela central para o RAG. Armazena o texto quebrado em pequenos trechos com seus respectivos timestamps (in√≠cio e fim).
* **embeddings:** Tabela vetorial que vincula cada segmento ao seu vetor representativo (embedding), permitindo a busca sem√¢ntica.

## üß† Busca Sem√¢ntica e RAG

A funcionalidade de busca (Retrieval-Augmented Generation) √© o cora√ß√£o do sistema:

1. **Vetoriza√ß√£o:** A pergunta do usu√°rio √© transformada em um vetor de embeddings usando o mesmo modelo da ingest√£o.
2. **Busca Vetorial:** O PostgreSQL utiliza o operador `<=>` (dist√¢ncia de cosseno/euclidiana) para encontrar os segmentos de transcri√ß√£o semanticamente mais pr√≥ximos da pergunta.
3. **Contexto:** Os trechos recuperados s√£o montados em um prompt de sistema ("Contexto").
4. **Gera√ß√£o:** O LLM recebe a pergunta e o contexto, gerando uma resposta natural baseada estritamente nas informa√ß√µes encontradas na m√≠dia.

## üèóÔ∏è Arquitetura

O sistema segue uma arquitetura distribu√≠da onde cada etapa do pipeline √© desacoplada e reage a eventos de dom√≠nio. Isso permite escalabilidade independente (ex: aumentar workers de transcri√ß√£o sem afetar a API de upload) e resili√™ncia.

### Fluxo de Dados

#### Diagrama de Arquitetura Geral (Microservi√ßos)

```mermaid
flowchart LR
    User --> UploadApi[Upload.Api]

    UploadApi -->|media.uploaded| RabbitMQ[(RabbitMQ)]

    RabbitMQ --> TranscriptionWorker[Transcription.Worker]
    TranscriptionWorker -->|media.transcribed| RabbitMQ

    RabbitMQ --> EmbeddingWorker[Embedding.Worker]
    EmbeddingWorker -->|media.embedded| RabbitMQ

    User --> QueryApi[Query.Api]

    UploadApi --> Postgres[(PostgreSQL + pgvector)]
    TranscriptionWorker --> Postgres
    EmbeddingWorker --> Postgres
    QueryApi --> Postgres

    QueryApi --> Ollama[(Ollama\nLLM + Embeddings)]
```

#### Pipeline da arquitetura orientada a eventos (Event-Driven)

```mermaid
sequenceDiagram
    participant U as User
    participant UA as Upload.Api
    participant MQ as RabbitMQ
    participant TW as Transcription.Worker
    participant EW as Embedding.Worker
    participant DB as PostgreSQL

    U ->> UA: Upload v√≠deo/√°udio
    UA ->> DB: Salva media
    UA ->> MQ: MediaUploadedEvent

    MQ ->> TW: MediaUploadedEvent
    TW ->> TW: Extrai √°udio (FFmpeg)
    TW ->> TW: Transcreve (Whisper)
    TW ->> DB: Salva transcri√ß√£o + segmentos
    TW ->> MQ: MediaTranscribedEvent

    MQ ->> EW: MediaTranscribedEvent
    EW ->> EW: Gera embeddings (Ollama)
    EW ->> DB: Salva embeddings (pgvector)
```

#### Modelo de Dados (Vis√£o Conceitual)

```mermaid
erDiagram
    MEDIA ||--o{ TRANSCRIPTIONS : has
    TRANSCRIPTIONS ||--o{ TRANSCRIPTION_SEGMENTS : contains
    TRANSCRIPTION_SEGMENTS ||--o{ EMBEDDINGS : generates

    MEDIA {
        uuid id
        string file_name
        string content_type
    }

    TRANSCRIPTIONS {
        uuid id
        uuid media_id
        text text
    }

    TRANSCRIPTION_SEGMENTS {
        uuid id
        uuid transcription_id
        int segment_index
        float start_seconds
        float end_seconds
        text text
    }

    EMBEDDINGS {
        uuid id
        uuid transcription_segment_id
        string model_name
        vector embedding
    }


```

## üõ† Como Executar o Projeto

### Pr√©-requisitos

* Docker e Docker Compose instalados.
* .NET 10 SDK (para desenvolvimento/build local).
* Ollama rodando localmente (ou configur√°vel via Docker) com os modelos necess√°rios (ex: `llama3`, `nomic-embed-text`).

### Passo a Passo

1. **Clone o reposit√≥rio**

    ```bash
    git clone https://github.com/seu-usuario/MediaTranscriptKnowledgeRAG.git
    cd MediaTranscriptKnowledgeRAG
    ```

2. **Suba a infraestrutura (RabbitMQ + Postgres)**

    ```bash
    docker-compose up -d
    ```

3. **Execute as aplica√ß√µes**
    Voc√™ pode rodar via Docker ou diretamente pelo .NET CLI:

    ```bash
    # Exemplo rodando a API de Upload
    dotnet run --project src/services/Upload.Api
    ```

4. **Acesse o Swagger**
    * Upload API: `https://localhost:7290/swagger`
    * Query API: `https://localhost:7032/swagger`

---

> **Nota:** Na primeira execu√ß√£o, o `Transcription.Worker` pode levar alguns instantes para baixar o modelo Whisper selecionado (se ainda n√£o estiver em cache).

## üìà Status e Evolu√ß√£o

### ‚úÖ Funcionalidades Atuais

* [x] Upload e armazenamento de arquivos.
* [x] Transcri√ß√£o offline com Whisper.
* [x] Segmenta√ß√£o temporal precisa.
* [x] Gera√ß√£o de Embeddings ass√≠ncrona.
* [x] Busca Sem√¢ntica (RAG) funcional.
* [x] Comunica√ß√£o com RabbitMQ
* [x] Arquitetura orientada a eventos  

### üöÄ Pr√≥ximos Passos (Roadmap)

* [ ] Interface de Usu√°rio (Web App).
* [ ] Suporte a m√∫ltiplos modelos de LLM via configura√ß√£o.
* [ ] Autentica√ß√£o e Multi-tenancy.
* [ ] Pipeline de reprocessamento de embeddings.
